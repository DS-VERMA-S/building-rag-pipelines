{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba278bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from  dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GORQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ad3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup LLM and Embedding models\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    groq_api_key=api_key,\n",
    "    temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6b0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class ProcessAndChunkPDFs:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Initialize PDF processor with chunking parameters.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Size of each text chunk\n",
    "            chunk_overlap: Overlap between chunks for context preservation\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def read_pdf(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Read PDF file and extract text with metadata.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing text and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pdf_reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            # Extract PDF metadata\n",
    "            metadata = {\n",
    "                \"source\": file_path,\n",
    "                \"file_name\": os.path.basename(file_path),\n",
    "                \"page_count\": page_count,\n",
    "                \"file_size\": os.path.getsize(file_path),\n",
    "                \"document_type\": \"PDF\"\n",
    "            }\n",
    "            \n",
    "            return {\"text\": text, \"metadata\": metadata}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {str(e)}\")\n",
    "            return {\"text\": \"\", \"metadata\": {}}\n",
    "    \n",
    "    def chunk_with_metadata(self, text: str, base_metadata: Dict[str, Any]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Chunk text and add metadata to each chunk.\n",
    "        \n",
    "        Args:\n",
    "            text: Full text to be chunked\n",
    "            base_metadata: Base metadata dictionary\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects with metadata\n",
    "        \"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_index\": chunk_num,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"total_chunks\": len(chunks)\n",
    "            }\n",
    "            \n",
    "            doc = Document(page_content=chunk, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_pdf_directory(self, directory_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process all PDFs in a directory.\n",
    "        \n",
    "        Args:\n",
    "            directory_path: Path to directory containing PDFs\n",
    "            \n",
    "        Returns:\n",
    "            List of chunked documents with metadata\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        pdf_files = Path(directory_path).glob(\"*.pdf\")\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing: {pdf_file.name}\")\n",
    "            \n",
    "            # Read PDF\n",
    "            pdf_data = self.read_pdf(str(pdf_file))\n",
    "            \n",
    "            if pdf_data[\"text\"]:\n",
    "                # Chunk with metadata\n",
    "                documents = self.chunk_with_metadata(\n",
    "                    pdf_data[\"text\"],\n",
    "                    pdf_data[\"metadata\"]\n",
    "                )\n",
    "                all_documents.extend(documents)\n",
    "                print(f\"  ✓ Extracted {len(documents)} chunks\")\n",
    "            else:\n",
    "                print(f\"  ✗ Failed to extract text\")\n",
    "        \n",
    "        return all_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize processor\n",
    "#     processor = PDFProcessor(chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "#     # Process single PDF\n",
    "#     single_pdf_path = \"path/to/your/document.pdf\"\n",
    "#     if os.path.exists(single_pdf_path):\n",
    "#         pdf_data = processor.read_pdf(single_pdf_path)\n",
    "#         documents = processor.chunk_with_metadata(pdf_data[\"text\"], pdf_data[\"metadata\"])\n",
    "        \n",
    "#         print(f\"\\nProcessed {len(documents)} chunks:\")\n",
    "#         for doc in documents[:2]:  # Print first 2 chunks\n",
    "#             print(f\"\\nMetadata: {doc.metadata}\")\n",
    "#             print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    \n",
    "#     # Process entire directory\n",
    "#     pdf_directory = \"path/to/pdf/directory\"\n",
    "#     if os.path.exists(pdf_directory):\n",
    "#         all_docs = processor.process_pdf_directory(pdf_directory)\n",
    "#         print(f\"\\n\\nTotal documents processed: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe6e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building-rag-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
