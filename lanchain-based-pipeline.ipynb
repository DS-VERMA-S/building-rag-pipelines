{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class ProcessAndChunkPDFs:\n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "    \n",
    "    def read_pdf(self, file_path: str) -> Dict[str, Any]:\n",
    "        \n",
    "        try:\n",
    "            pdf_reader = PdfReader(file_path)\n",
    "            text = \"\"\n",
    "            page_count = len(pdf_reader.pages)\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            # Extract PDF metadata\n",
    "            metadata = {\n",
    "                \"source\": file_path,\n",
    "                \"file_name\": os.path.basename(file_path),\n",
    "                \"page_count\": page_count,\n",
    "                \"file_size\": os.path.getsize(file_path),\n",
    "                \"document_type\": \"PDF\"\n",
    "            }\n",
    "            \n",
    "            return {\"text\": text, \"metadata\": metadata}\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {str(e)}\")\n",
    "            return {\"text\": \"\", \"metadata\": {}}\n",
    "    \n",
    "    def chunk_with_metadata(self, text: str, base_metadata: Dict[str, Any]) -> List[Document]:\n",
    "        \n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        for chunk_num, chunk in enumerate(chunks):\n",
    "            metadata = {\n",
    "                **base_metadata,\n",
    "                \"chunk_index\": chunk_num,\n",
    "                \"chunk_size\": len(chunk),\n",
    "                \"total_chunks\": len(chunks)\n",
    "            }\n",
    "            \n",
    "            doc = Document(page_content=chunk, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_pdf_directory(self, directory_path: str) -> List[Document]:\n",
    "        \n",
    "        all_documents = []\n",
    "        pdf_files = Path(directory_path).glob(\"*.pdf\")\n",
    "        \n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing: {pdf_file.name}\")\n",
    "            \n",
    "            # Read PDF\n",
    "            pdf_data = self.read_pdf(str(pdf_file))\n",
    "            \n",
    "            if pdf_data[\"text\"]:\n",
    "                # Chunk with metadata\n",
    "                documents = self.chunk_with_metadata(\n",
    "                    pdf_data[\"text\"],\n",
    "                    pdf_data[\"metadata\"]\n",
    "                )\n",
    "                all_documents.extend(documents)\n",
    "                print(f\" Extracted {len(documents)} chunks\")\n",
    "            else:\n",
    "                print(f\"Failed to extract text\")\n",
    "        \n",
    "        return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = ProcessAndChunkPDFs(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "directory_path = \"C:\\\\Users\\\\sachi\\\\Projects\\\\PythonProjects\\\\building-rag-pipelines\\\\PDF_Data\"\n",
    "if os.path.exists(directory_path):\n",
    "    all_docs = processor.process_pdf_directory(directory_path)\n",
    "    documents = all_docs\n",
    "    print(f\"\\n\\nTotal documents processed: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba278bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from  dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GORQ_API_KEY\")\n",
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "llm = ChatGroq(\n",
    "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    groq_api_key=api_key,\n",
    "    temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2b263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "786606d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
    "retrived_docs  = [x.page_content for x in retriver.invoke(\"What is chain of thought Hijacking?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35d1b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breath and work on this problem step-by-step...‚Äù).\n",
      "Table 1: Attack success rate (ASR, %) on S1 under different CoT length conditions.\n",
      "Setting Minimal Natural Extended\n",
      "ASR (%) 27 51 80\n",
      "The results reveal a clear pattern: longer reasoning traces substantially increase the likelihood of\n",
      "harmful outputs, with ASR rising from 27% (Minimal) to 80% (Extended). This provides an initial\n",
      "behavioral clue that refusals in reasoning models degrade as CoT length grows. In Section 4, we\n",
      "build on this observation by introducingChain-of-Thought Hijacking, a systematic jailbreak that\n",
      "exploits this vulnerability.\n",
      "4 CHAIN-OF-THOUGHTHIJACKING: ATTACKDESIGN ANDEMPIRICAL\n",
      "RESULTS\n",
      "4.1 JAILBREAK METHODOLOGY\n",
      "We define a prompt-based jailbreak,CoT Hijacking. The attack prepends a long, benign reasoning\n",
      "preface to a harmful instruction, followed by a final-answer cue. This structure systematically re-\n",
      "duces refusals: the benign CoT dilutes the refusal signal while the cue shifts attention to the answer\n",
      "region. \n",
      "We introducedCoT Hijacking, a simple jailbreak attack against reasoning models. By padding\n",
      "harmful requests with long benign reasoning and a final-answer cue, we showed that refusal signals\n",
      "are diluted, resulting in high attack success rates across both open and proprietary LRMs. Unlike\n",
      "prior attacks that rely on visible safety reasoning or disguises, our method exploits a more funda-\n",
      "mental weakness: safety checks depend on residual activations that become less discriminative as\n",
      "CoT length increases.\n",
      "Through mechanistic analysis we found refusal components encode both thestrengthof safety\n",
      "checking in middle layers and theoutcomeof verification in later layers. Long CoT hijacking\n",
      "suppresses these signals, shifting attention away from harmful tokens and flattening refusal direc-\n",
      "tions. Interventions on targeted attention heads confirmed their causal role, showing that hijacking\n",
      "undermines a specific safety subnetwork. These results have two implications. First, reasoning mod-\n"
     ]
    }
   ],
   "source": [
    "print(\" \\n\".join(retrived_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building-rag-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
